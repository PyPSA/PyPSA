{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5f8f8",
   "metadata": {
    "tags": [
     "injected-warnings",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# General notebook settings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"error\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596305f2",
   "metadata": {},
   "source": [
    "# Time Series Aggregation\n",
    "\n",
    "In this example, we are going to explore different ways to cluster the temporal resolution of PyPSA models, and what impact they have on the optimisation results and solving times. Using an hourly resolved variant of the [single-node capacity expansion example](), we will compare three different approaches to reduce the number of time steps in the model:\n",
    "\n",
    "- **Sampling**: Selecting a subset of the given snapshots based on a given frequency,\n",
    "- **Averaging**: Aggregating the snapshots by averaging them over a given frequency, and\n",
    "- **Segmentation**: Clustering the snapshots into segments of a given frequency and using [`tsam`](https://tsam.readthedocs.io/en/latest/) library.\n",
    "\n",
    "We start with the usual imports and loading the hourly resolved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef920172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsam.timeseriesaggregation as tsam\n",
    "\n",
    "import pypsa\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "SOLVER = \"highs\"  # or \"gurobi\"\n",
    "\n",
    "template_n = pypsa.Network(\n",
    "    \"https://tubcloud.tu-berlin.de/s/4ra3NKrLGzE42of/download/model-energy-hourly.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bcabc",
   "metadata": {},
   "source": [
    "Since we also want to monitor the solving times, we will use a small utility function that wraps around the solving process and returns the seconds it took to solve the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(func, *args, **kwargs):\n",
    "    \"\"\"Time the execution of a function and return the elapsed time in seconds.\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return result, elapsed_time\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a975e1e",
   "metadata": {},
   "source": [
    "## Hourly Baseline\n",
    "\n",
    "Additionally, we need a baseline model to compare aggregated models against. We will use the hourly resolved model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hourly = template_n.copy()\n",
    "\n",
    "_, s_hourly = time_it(n_hourly.optimize, solver_name=SOLVER, log_to_console=False)\n",
    "s_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7526bc",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "We will start with the sampling approach, which is the simplest one. We simply select every $N$-th snapshot from the model. The important part here is that we need to adjust the snapshot weightings accordingly, as each remaining snapshot now represents $N$ hours. We iterate over N from 2 to 11, i.e. from 2-hourly to 11-hourly resolved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a019dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_n = {1: n_hourly}\n",
    "sampling_s = {1: s_hourly}\n",
    "\n",
    "for resolution in range(2, 12):\n",
    "    n = template_n.copy()\n",
    "\n",
    "    # set the sampled snapshots (time series are automatically reduced)\n",
    "    n.set_snapshots(n.snapshots[::resolution])\n",
    "    n.snapshot_weightings.loc[:, :] = resolution\n",
    "\n",
    "    _, s = time_it(n.optimize, solver_name=SOLVER, log_to_console=False)\n",
    "\n",
    "    sampling_n[resolution] = n\n",
    "    sampling_s[resolution] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767da0f",
   "metadata": {},
   "source": [
    "## Averaging\n",
    "\n",
    "The averaging approach also has equal snapshot durations, but instead of selecting data from every $N$-th snapshots, we average time series data for every $N$ snapshots. This means that the resulting model has $N$ times fewer snapshots, but each snapshot represents the average of $N$ original snapshots. Again, we need to adjust the snapshot weightings accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaging_n = {1: n_hourly}\n",
    "averaging_s = {1: s_hourly}\n",
    "\n",
    "for resolution in range(2, 12):\n",
    "    n = template_n.copy()\n",
    "\n",
    "    # resample the time series data by averaging\n",
    "    n.loads_t.p_set = n.loads_t.p_set.resample(f\"{resolution}h\").mean()\n",
    "    n.generators_t.p_max_pu = n.generators_t.p_max_pu.resample(f\"{resolution}h\").mean()\n",
    "\n",
    "    # set the new snapshtos and adjusted snapshot weightings\n",
    "    n.set_snapshots(n.snapshots[::resolution])\n",
    "    n.snapshot_weightings.loc[:, :] = resolution\n",
    "\n",
    "    _, s = time_it(n.optimize, solver_name=SOLVER, log_to_console=False)\n",
    "\n",
    "    averaging_n[resolution] = n\n",
    "    averaging_s[resolution] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6677b7",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "The segmentation approach is more complex. It uses a separate library called [`tsam`](https://tsam.readthedocs.io/en/latest/) to cluster the snapshots into segments of varying lengths. The sequence of snapshots is preserved as segments are only formed from neighbouring snapshots based on their similarity. For measuring similarity, it is advisable to normalise the time series data. This approach promises to capture the temporal patterns more effectively, as it can opt for higher resolution during periods of high variability and lower resolution during periods with low variability. The snapshot weightings are adjusted based on the number of snapshots in each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46398e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_n = {1: n_hourly}\n",
    "segmentation_s = {1: s_hourly}\n",
    "\n",
    "for resolution in range(2, 12):\n",
    "    n = template_n.copy()\n",
    "\n",
    "    # calculate number of segments equivalent to resolution\n",
    "    segments = int(8760 / resolution)\n",
    "\n",
    "    # concatenate and normalize all time series with min-max normalization\n",
    "    df = pd.concat([n.generators_t.p_max_pu, n.loads_t.p_set], axis=1)\n",
    "    df_norm = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # use `tsam` to run segmentation clustering algorithm\n",
    "    agg = tsam.TimeSeriesAggregation(\n",
    "        df_norm,\n",
    "        hoursPerPeriod=len(df_norm),\n",
    "        noTypicalPeriods=1,\n",
    "        noSegments=segments,\n",
    "        segmentation=True,\n",
    "        solver=SOLVER,\n",
    "    )\n",
    "    agg = agg.createTypicalPeriods()\n",
    "\n",
    "    # translate segments into time stamps and calculate new weightings\n",
    "    weightings = agg.index.get_level_values(\"Segment Duration\")\n",
    "    offsets = np.insert(np.cumsum(weightings[:-1]), 0, 0)\n",
    "    weightings = n.snapshot_weightings.loc[n.snapshots[offsets]].mul(weightings, axis=0)\n",
    "\n",
    "    # aggregate the hourly time series by averaging over the segments\n",
    "    mapping = (\n",
    "        pd.Series(weightings.index, index=weightings.index).reindex(n.snapshots).ffill()\n",
    "    )\n",
    "    n.generators_t.p_max_pu = n.generators_t.p_max_pu.groupby(mapping).mean()\n",
    "    n.loads_t.p_set = n.loads_t.p_set.groupby(mapping).mean()\n",
    "\n",
    "    # set new segmented snapshots and adjust weightings\n",
    "    n.set_snapshots(weightings.index)\n",
    "    n.snapshot_weightings = weightings\n",
    "\n",
    "    # run optimization\n",
    "    _, s = time_it(n.optimize, solver_name=SOLVER, log_to_console=False)\n",
    "\n",
    "    segmentation_n[resolution] = n\n",
    "    segmentation_s[resolution] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ce5e4",
   "metadata": {},
   "source": [
    "Now before we go ahead with the evaluation of the different approaches, let's quickly glance at the distribution of snapshot durations obtained from the segmentation approach for a resolution equivalent to a 3-hourly model. We can see quite some variability in the segment length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ef52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_n[3].snapshot_weightings.generators.value_counts().sort_index(\n",
    "    ascending=True\n",
    ").plot.bar(ylabel=\"snapshots [number]\", xlabel=\"snapshot duration [h]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e9675",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's start our evaluation with a look at the solving times. We can see that across all approaches, the solving times quickly decay, especially as we go from hourly to 2-hourly resolved models and decrease less substantially afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sampling_s).plot(label=\"sampling\", legend=True)\n",
    "pd.Series(averaging_s).plot(label=\"averaging\", legend=True)\n",
    "pd.Series(segmentation_s).plot(\n",
    "    label=\"segmentation\", ylabel=\"time [s]\", xlabel=\"resolution [h]\", legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98287aea",
   "metadata": {},
   "source": [
    "Furthermore, we compare the relative error in total system costs compared to the hourly resolved model. We can see how the segmentation approach remains more stable than the other two approaches, especially for lower resolutions. Even with 11-hourly equivalent resolution, the segmentation approach only has a relative error of -1% compared to the hourly resolved model (i.e. it appears to be 1% cheaper). Similar patterns can be observed for the relative error in the total installed capacity of solar and batteries, two technologies that are particularly sensitive to the temporal resolution of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4848f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsc(n):\n",
    "    return (n.statistics.capex().sum() + n.statistics.opex().sum()) / 1e9\n",
    "\n",
    "\n",
    "pd.concat(\n",
    "    [\n",
    "        pd.Series({(\"sampling\", res): tsc(n) for res, n in sampling_n.items()}),\n",
    "        pd.Series({(\"averaging\", res): tsc(n) for res, n in averaging_n.items()}),\n",
    "        pd.Series({(\"segmentation\", res): tsc(n) for res, n in segmentation_n.items()}),\n",
    "    ]\n",
    ").unstack(0).div(tsc(n_hourly)).sub(1).mul(100).plot(\n",
    "    ylabel=\"relative objective error [%]\", xlabel=\"resolution [h]\", legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar(n):\n",
    "    return n.generators.loc[\"solar\", \"p_nom_opt\"]\n",
    "\n",
    "\n",
    "pd.concat(\n",
    "    [\n",
    "        pd.Series({(\"sampling\", res): solar(n) for res, n in sampling_n.items()}),\n",
    "        pd.Series({(\"averaging\", res): solar(n) for res, n in averaging_n.items()}),\n",
    "        pd.Series(\n",
    "            {(\"segmentation\", res): solar(n) for res, n in segmentation_n.items()}\n",
    "        ),\n",
    "    ]\n",
    ").unstack(0).div(solar(n_hourly)).sub(1).mul(100).plot(\n",
    "    ylabel=\"relative solar capacity error [%]\", xlabel=\"resolution [h]\", legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba10074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def battery(n):\n",
    "    return n.storage_units.loc[\"battery storage\", \"p_nom_opt\"]\n",
    "\n",
    "\n",
    "pd.concat(\n",
    "    [\n",
    "        pd.Series({(\"sampling\", res): battery(n) for res, n in sampling_n.items()}),\n",
    "        pd.Series({(\"averaging\", res): battery(n) for res, n in averaging_n.items()}),\n",
    "        pd.Series(\n",
    "            {(\"segmentation\", res): battery(n) for res, n in segmentation_n.items()}\n",
    "        ),\n",
    "    ]\n",
    ").unstack(0).div(battery(n_hourly)).sub(1).mul(100).plot(\n",
    "    ylabel=\"relative battery capacity error [%]\", xlabel=\"resolution [h]\", legend=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
